{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 토큰화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\"\n",
    "\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.split(r'[-\\s.,;!?]', sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text1 = \"Hello! I can't wait to try the word_tokenize, WordPunctTokenizer, and TreebankWordTokenizer.\"\n",
    "text2 = \"They'll save and re-use this file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'ca', \"n't\", 'wait', 'to', 'try', 'the', 'word_tokenize', ',', 'WordPunctTokenizer', ',', 'and', 'TreebankWordTokenizer', '.']\n",
      "['They', \"'ll\", 'save', 'and', 're-use', 'this', 'file', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokenize1 = word_tokenize(text1)\n",
    "word_tokenize2 = word_tokenize(text2)\n",
    "\n",
    "print(word_tokenize1)\n",
    "print(word_tokenize2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'can', \"'t\", 'wait', 'to', 'try', 'the', 'word_tokenize', ',', 'WordPunctTokenizer', ',', 'and', 'TreebankWordTokenizer', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'can', \"'\", 't', 'wait', 'to', 'try', 'the', 'word_tokenize', ',', 'WordPunctTokenizer', ',', 'and', 'TreebankWordTokenizer', '.']\n",
      "['They', \"'\", 'll', 'save', 'and', 're', '-', 'use', 'this', 'file', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "word_punc_token = WordPunctTokenizer()\n",
    "print(word_punc_token.tokenize(text1))\n",
    "print(word_punc_token.tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TreebankWordTokenizer (RegexpTokenizer보다 강력)\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# 예제 text \n",
    "sentence = \"\"\"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\"\"\n",
    "sentence2 = \"Hello! I can't wait to try the word_tokenize, WordPunctTokenizer, and TreebankWordTokenizer.\"\n",
    "sentence3 = \"They'll save and re-use this file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Monticello', 'was', \"n't\", 'designated', 'as', 'UNESCO', 'World', 'Heritage', 'Site', 'until', '1987', '.']\n",
      "['Hello', '!', 'I', 'ca', \"n't\", 'wait', 'to', 'try', 'the', 'word_tokenize', ',', 'WordPunctTokenizer', ',', 'and', 'TreebankWordTokenizer', '.']\n",
      "['They', \"'ll\", 'save', 'and', 're-use', 'this', 'file', '.']\n"
     ]
    }
   ],
   "source": [
    "tree_bank = TreebankWordTokenizer()\n",
    "\n",
    "print(tree_bank.tokenize(sentence))\n",
    "print(tree_bank.tokenize(sentence2))\n",
    "print(tree_bank.tokenize(sentence3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@TJMonticello',\n",
       " 'Best',\n",
       " 'day',\n",
       " 'everrrrrrr',\n",
       " 'at',\n",
       " 'Monticello',\n",
       " '.',\n",
       " 'Awesommmmmeeeeee',\n",
       " 'day',\n",
       " ':*)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import casual_tokenize\n",
    "\n",
    "message = \"\"\"RT @TJMonticello Best day everrrrrrr at Monticello.\\\n",
    "    Awesommmmmeeeeee day :*)\"\"\"\n",
    "\n",
    "casual_tokenize(message, reduce_len = True, strip_handles=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 2 - 문서를 토큰화한 후 불용어 제거해보기 \n",
    "\n",
    "# 불용어 리스트 (불용어로 간주할 단어들)\n",
    "stopwords = ['the', 'is', 'in', 'and', 'to', 'a', 'of']\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog. The dog barked loudly at the fox in the park.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'jumps',\n",
       " 'over',\n",
       " 'lazy',\n",
       " 'dog',\n",
       " '.',\n",
       " 'The',\n",
       " 'dog',\n",
       " 'barked',\n",
       " 'loudly',\n",
       " 'at',\n",
       " 'fox',\n",
       " 'park',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "result = [token for token in tokens if token not in stopwords]\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dish', 'washer', 'wash', 'dish']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "[stemmer.stem(token).strip(\"'\") for token in \"dish washer's washed dishes\".split()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dish washer wash dish'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([stemmer.stem(token).strip(\"'\") for token in \"dish washer's washed dishes\".split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -> run\n",
      "beautiful -> beauti\n",
      "believes -> believ\n",
      "using -> use\n",
      "conversation -> convers\n",
      "organization -> organ\n",
      "studies -> studi\n"
     ]
    }
   ],
   "source": [
    "# running, beautiful, believes, using, conversation, organization, studies 원형 복원\n",
    "print(\"running -> \" + stemmer.stem(\"running\"))\n",
    "print(\"beautiful -> \" + stemmer.stem(\"beautiful\"))\n",
    "print(\"believes -> \" + stemmer.stem(\"believes\"))\n",
    "print(\"using -> \" + stemmer.stem(\"using\"))\n",
    "print(\"conversation -> \" + stemmer.stem(\"conversation\"))\n",
    "print(\"organization -> \" + stemmer.stem(\"organization\"))\n",
    "print(\"studies -> \" + stemmer.stem(\"studies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer.lemmatize('better', pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동사 형태 : running -> run\n",
      "형용사 형태 : beautiful -> beautiful\n",
      "명사 형태 : geese -> goose\n"
     ]
    }
   ],
   "source": [
    "# 품사를 명시하여 lemmatize 적용\n",
    "from nltk.corpus import wordnet\n",
    "print(\"동사 형태 : running -> \" + lemmatizer.lemmatize(\"running\", pos=wordnet.VERB))  # 동사 형태로 추출\n",
    "print(\"형용사 형태 : beautiful -> \" + lemmatizer.lemmatize(\"beautiful\", pos=wordnet.ADJ))  # 형용사 형태로 추출\n",
    "print(\"명사 형태 : geese -> \" + lemmatizer.lemmatize(\"geese\", pos=wordnet.NOUN))  # 명사 형태로 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "result = komoran.morphs(kor_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인간', '이', '컴퓨터', '와', '대화', '하고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "result = hannanum.morphs(kor_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus(말뭉치)\n",
    "\n",
    "docs = ['오늘 동물원에서 원숭이를 봤어',\n",
    "        '오늘 동물원에서 코끼리를 봤어 봤어',\n",
    "        '동물원에서 원숭이에게 바나나를 줬어 바나나를']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 1 0 0 0]\n",
      " [1 0 2 1 0 0 0 1]\n",
      " [1 2 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "BoW = count_vect.fit_transform(docs)\n",
    "\n",
    "print(BoW.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['동물원에서' '바나나를' '봤어' '오늘' '원숭이를' '원숭이에게' '줬어' '코끼리를']\n"
     ]
    }
   ],
   "source": [
    "# df화\n",
    "\n",
    "# 어휘집 생성\n",
    "vocab = count_vect.get_feature_names_out()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 오늘 동물원에서 원숭이를 봤어\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>동물원에서</th>\n",
       "      <th>바나나를</th>\n",
       "      <th>봤어</th>\n",
       "      <th>오늘</th>\n",
       "      <th>원숭이를</th>\n",
       "      <th>원숭이에게</th>\n",
       "      <th>줬어</th>\n",
       "      <th>코끼리를</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   동물원에서  바나나를  봤어  오늘  원숭이를  원숭이에게  줬어  코끼리를\n",
       "0      1     0   1   1     1      0   0     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "1 : 오늘 동물원에서 코끼리를 봤어 봤어\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>동물원에서</th>\n",
       "      <th>바나나를</th>\n",
       "      <th>봤어</th>\n",
       "      <th>오늘</th>\n",
       "      <th>원숭이를</th>\n",
       "      <th>원숭이에게</th>\n",
       "      <th>줬어</th>\n",
       "      <th>코끼리를</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   동물원에서  바나나를  봤어  오늘  원숭이를  원숭이에게  줬어  코끼리를\n",
       "0      1     0   2   1     0      0   0     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "2 : 동물원에서 원숭이에게 바나나를 줬어 바나나를\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>동물원에서</th>\n",
       "      <th>바나나를</th>\n",
       "      <th>봤어</th>\n",
       "      <th>오늘</th>\n",
       "      <th>원숭이를</th>\n",
       "      <th>원숭이에게</th>\n",
       "      <th>줬어</th>\n",
       "      <th>코끼리를</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   동물원에서  바나나를  봤어  오늘  원숭이를  원숭이에게  줬어  코끼리를\n",
       "0      1     2   0   0     0      1   1     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "for i in range (len(docs)):\n",
    "    print('{} : {}'.format(i, docs [i]))\n",
    "\n",
    "    display(pd.DataFrame([BoW.toarray()[i]], columns=vocab))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['오늘 동물원에서 원숭이를 봤어',\n",
    "        '오늘 동물원에서 코끼리를 봤어 봤어',\n",
    "        '동물원에서 원숭이에게 바나나를 줬어 바나나를']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "doc_ls = [doc.split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
