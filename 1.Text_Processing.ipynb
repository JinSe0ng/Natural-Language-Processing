{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /Users/ijinseong/.conda/envs/sesac/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리 (Text Preprocessing)\n",
    "*   텍스트를 자연어 처리를 위해 용도에 맞도록 사전에 표준화 하는 작업\n",
    "*   텍스트 내 정보를 유지하고, 중복을 제거하여 분석 효율성을 높이기 위해 전처리를 수행\n",
    "\n",
    "### 1) 토큰화 (Tokenizing)\n",
    "* 토큰화는 단어별로 분리하는 \"단어 토큰화(Word Tokenization)\"와 문장별로 분리하는 \"문장 토큰화(Sentence Tokenization)\"로 구분\n",
    "(이후 실습에서는 단어 토큰화를 \"토큰화\"로 통일)\n",
    "\n",
    "### 2) 품사 부착(PoS Tagging)\n",
    "* 각 토큰에 품사 정보를 추가\n",
    "* 분석시에 불필요한 품사를 제거하거나 (예. 조사, 접속사 등) 필요한 품사를 필터링 하기 위해 사용\n",
    "\n",
    "### 3) 개체명 인식 (NER, Named Entity Recognition)\n",
    "* 각 토큰의 개체 구분(기관, 인물, 지역, 날짜 등) 태그를 부착\n",
    "* 텍스트가 무엇과 관련되어있는지 구분하기 위해 사용\n",
    "* 예를 들어, 과일의 apple과 기업의 apple을 구분하는 방법이 개체명 인식임\n",
    "\n",
    "### 4) 원형 복원 (Stemming & Lemmatization)\n",
    "* 각 토큰의 원형 복원을 함으로써 토큰을 표준화하여 불필요한 데이터 중복을 방지 (=단어의 수를 줄일수 있어 연산을 효율성을 높임)\n",
    "* 어간 추출(Stemming) : 품사를 무시하고 규칙에 기반하여 어간을 추출\n",
    "* 표제어 추출 (Lemmatization) : 품사정보를 유지하여 표제어 추출\n",
    "\n",
    "### 5) 불용어 처리 (Stopword)\n",
    "* 자연어 처리를 위해 불필요한 요소를 제거하는 작업\n",
    "* 불필요한 품사를 제거하는 작업과 불필요한 단어를 제거하는 작업으로 구성\n",
    "* 불필요한 토큰을 제거함으로써 연산의 효율성을 높임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 영문 전처리 실습\n",
    "\n",
    "- NLTK 토크나이저 사용\n",
    "  - 교육용으로 개발된 자연어 처리 및 문서 분석용 파이썬 패키지\n",
    "\n",
    "- [NLTK](https://www.nltk.org) lib 사용\n",
    "- [영문 토큰화](https://www.nltk.org/api/nltk.tokenize.html) 모듈\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 str.split() 메소드 통한 예제 문장 토큰화\n",
    "\n",
    "- 몬티첼로 예제 문장을 토큰들로 분할 \n",
    "    - 몬티첼로는 미국의 건국의 아버지 중 한 명인 토머스 제퍼슨 대통령이 젊은 시절 설계하고 건축에도 직접 관여한 저택의 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\"\n",
    "\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.split(sentence)  #str 클래스에서 제공하는 split()메서드 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 토큰 개선\n",
    "- 26.처럼 토큰에 후행 마침표가 붙은 경우 후행 마침표 없이 토큰화 하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\"\n",
    "tokens = re.split(r'[-\\s.,;!?]+', sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 RegexpTokenizer()\n",
    "- 공백을 제거해준다는 점에서 단순 정규식으로 토큰화 하는 것 보다 성능이 조금 더 좋음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "tokenizer.tokenize(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 word_tokenize()\n",
    "- 마침표와 구두점(온점(.), 컴마(,), 물음표(?), 세미콜론(;), 느낌표(!) 등과 같은 기호)으로 구분하여 토큰화\n",
    "- 내부적으로 TreebankWordTokenizer를 사용하기 때문에 결과가 동일한 경우가 많음\n",
    "- NLTK에서 제공하는 고수준 함수이며 더 나은 추상화와 호환성 제공\n",
    "    - 고수준: 사용자가 쉽게 이해하고 사용할 수 있도록 추상화된 인터페이스 제공 \n",
    "    - 즉, 이해하기 쉬운 함수 하나로 여러가징 기능 수행 가능 \n",
    "\n",
    "### 1.1.5 nltk.download('punkt')\n",
    "- punkt는 언어 독립적인 문장 분할기를 포함하고 있어, 텍스트를 문장 단위로 나누는 데 사용.\n",
    "- 예를 들어, 문단 내에서 개별 문장을 분리하거나, 문장 끝을 인식하는 작업에 활용됩니다.\n",
    "- Punkt 토크나이저는 규칙 기반이 아닌 비지도 학습 방법으로 학습되어 있어 다양한 언어와 문장 구조에 잘 적용될 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/ijinseong/nltk_data',\n",
       " '/Users/ijinseong/.conda/envs/sesac/nltk_data',\n",
       " '/Users/ijinseong/.conda/envs/sesac/share/nltk_data',\n",
       " '/Users/ijinseong/.conda/envs/sesac/lib/nltk_data',\n",
       " '/usr/share/nltk_data',\n",
       " '/usr/local/share/nltk_data',\n",
       " '/usr/lib/nltk_data',\n",
       " '/usr/local/lib/nltk_data',\n",
       " '/Users/{사용자 이름}}/nltk_data',\n",
       " '/Users/ijinseong/nltk_data',\n",
       " '/Users/ijinseong/nltk_data',\n",
       " './Users/ijinseong/nltk_data']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 경로 수정\n",
    "path = './Users/ijinseong/nltk_data'\n",
    "nltk.data.path.append(path)\n",
    "\n",
    "# 마지막으로 추가한 경로 삭제 \n",
    "# nltk.data.path.pop()\n",
    "\n",
    "# 추가돼있는 경로 확인 \n",
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     ./Users/ijinseong/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     ./Users/ijinseong/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 패키지 다운로드 \n",
    "nltk.download('punkt_tab', download_dir=path)\n",
    "nltk.download('punkt', download_dir=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text1 = \"Hello! I can't wait to try the word_tokenize, WordPunctTokenizer, and TreebankWordTokenizer.\"\n",
    "text2 = \"They'll save and re-use this file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'ca', \"n't\", 'wait', 'to', 'try', 'the', 'word_tokenize', ',', 'WordPunctTokenizer', ',', 'and', 'TreebankWordTokenizer', '.']\n",
      "['They', \"'ll\", 'save', 'and', 're-use', 'this', 'file', '.']\n"
     ]
    }
   ],
   "source": [
    "# 'll을 하나의 토큰으로 묶음, 구두점도 하나의 토큰으로 묶음 \n",
    "# TreebankWordToknizer 기반이기 때문에 축약형인 n't를 하나의 토큰으로 분리해냄\n",
    "word_tokens1 = word_tokenize(text1)\n",
    "word_tokens2 = word_tokenize(text2)\n",
    "\n",
    "print(word_tokens1)\n",
    "print(word_tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6 WordPunctTokenizer()  \n",
    "- 알파벳이 아닌 문자를 구분하여 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "text1 = \"Hello! I can't wait to try the word_tokenize, WordPunctTokenizer, and TreebankWordTokenizer.\"\n",
    "text2 = \"They'll save and re-use this file.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'can', \"'\", 't', 'wait', 'to', 'try', 'the', 'word_tokenize', ',', 'WordPunctTokenizer', ',', 'and', 'TreebankWordTokenizer', '.']\n",
      "['They', \"'\", 'll', 'save', 'and', 're', '-', 'use', 'this', 'file', '.']\n"
     ]
    }
   ],
   "source": [
    "wordpunctoken1 = WordPunctTokenizer().tokenize(text1)\n",
    "wordpunctoken2 = WordPunctTokenizer().tokenize(text2)\n",
    "\n",
    "print(wordpunctoken1)\n",
    "print(wordpunctoken2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.7 TreebankWordTokenizer()\n",
    "- Penn Treebank에서 사용하는 규칙에 따라 토큰 분리\n",
    "- 구두점을 분리하고 **축약형에서 \"n't\"를 분리**하는 특징이 있음\n",
    "- 영어 단어 토큰화에 흔히 쓰이는 다양한 규칙을 담고 있음\n",
    "    - 문장 끝 부호 (?!.;,)를 인접 토큰들과 분리\n",
    "    - 소수점이 있는 수치는 하나의 토큰으로 유지 \n",
    "    - 축약형 단어들(n't 같은..)을 위한 규칙도 갖고 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TreebankWordTokenizer (RegexpTokenizer보다 강력)\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# 예제 text \n",
    "sentence = \"\"\"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\"\"\n",
    "sentence2 = \"Hello! I can't wait to try the word_tokenize, WordPunctTokenizer, and TreebankWordTokenizer.\"\n",
    "sentence3 = \"They'll save and re-use this file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Monticello', 'was', \"n't\", 'designated', 'as', 'UNESCO', 'World', 'Heritage', 'Site', 'until', '1987', '.']\n",
      "['Hello', '!', 'I', 'ca', \"n't\", 'wait', 'to', 'try', 'the', 'word_tokenize', ',', 'WordPunctTokenizer', ',', 'and', 'TreebankWordTokenizer', '.']\n",
      "['They', \"'ll\", 'save', 'and', 're-use', 'this', 'file', '.']\n"
     ]
    }
   ],
   "source": [
    "treebankwordtoken = TreebankWordTokenizer().tokenize(sentence)\n",
    "print(treebankwordtoken)\n",
    "\n",
    "treebankwordtoken2 = TreebankWordTokenizer().tokenize(sentence2)\n",
    "print(treebankwordtoken2)\n",
    "\n",
    "treebankwordtoken3 = TreebankWordTokenizer().tokenize(sentence3)\n",
    "print(treebankwordtoken3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.8 casual_tokenize()\n",
    "- casual_tokenize: sns에서 얻으 비형식적 텍스트(이모티콘이 난무한다던가)를 토큰화 하기에 유용 \n",
    "- 텍스트에서 사용자 이름을 제거하고 토큰 안에서 반복되는 문자들을 줄이는 데 유용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@TJMonticello',\n",
       " 'Best',\n",
       " 'day',\n",
       " 'everrrrrrr',\n",
       " 'at',\n",
       " 'Monticello',\n",
       " '.',\n",
       " 'Awesommmmmeeeeee',\n",
       " 'day',\n",
       " ':*)']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import casual_tokenize\n",
    "\n",
    "message = \"\"\"RT @TJMonticello Best day everrrrrrr at Monticello.\\\n",
    "    Awesommmmmeeeeee day :*)\"\"\"\n",
    "casual_tokenize(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " 'Best',\n",
       " 'day',\n",
       " 'everrr',\n",
       " 'at',\n",
       " 'Monticello',\n",
       " '.',\n",
       " 'Awesommmeee',\n",
       " 'day',\n",
       " ':*)']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casual_tokenize(message, reduce_len=True, strip_handles=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.9 n-gram \n",
    "- 축약형 문제 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentence = \"Thomas Jefferson began building Monticello at the age of 26.\"\n",
    "\n",
    "pattern = re.compile(r'([-\\s.,;!?])+')\n",
    "tokens = pattern.split(sentence)\n",
    "\n",
    "tokens = [x for x in tokens if x and x not in '- \\t\\n.,;!?']\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x10d82b4c0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "# 2-gram 생성\n",
    "ngrams(tokens,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thomas', 'Jefferson', 'began'),\n",
       " ('Jefferson', 'began', 'building'),\n",
       " ('began', 'building', 'Monticello'),\n",
       " ('building', 'Monticello', 'at'),\n",
       " ('Monticello', 'at', 'the'),\n",
       " ('at', 'the', 'age'),\n",
       " ('the', 'age', 'of'),\n",
       " ('age', 'of', '26')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 불용어 (Stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 단어들로 불용어 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['house', 'fire']\n"
     ]
    }
   ],
   "source": [
    "# 예제1\n",
    "\n",
    "# 불용어 리스트 (불용어로 간주할 단어들)\n",
    "stop_words = ['a', 'an', 'the', 'on', 'of', 'off', 'this', 'is']\n",
    "tokens = ['the', 'house', 'is', 'on', 'fire']\n",
    "\n",
    "tokens_without_stopwords = [token for token in tokens if token not in stop_words]\n",
    "print(tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 2 - 문서를 토큰화한 후 불용어 제거해보기 \n",
    "\n",
    "# 불용어 리스트 (불용어로 간주할 단어들)\n",
    "stopwords = ['the', 'is', 'in', 'and', 'to', 'a', 'of']\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog. The dog barked loudly at the fox in the park.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'The', 'dog', 'barked', 'loudly', 'at', 'the', 'fox', 'in', 'the', 'park', '.']\n"
     ]
    }
   ],
   "source": [
    "# 텍스트를 단어 단위로 토큰화\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "filtered_tokens = [token for token in tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 토큰:  ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'The', 'dog', 'barked', 'loudly', 'at', 'the', 'fox', 'in', 'the', 'park', '.']\n",
      "불용어 제거 후 토큰:  ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog', '.', 'The', 'dog', 'barked', 'loudly', 'at', 'fox', 'park', '.']\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력\n",
    "print('원래 토큰: ',tokens)\n",
    "print('불용어 제거 후 토큰: ', filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 POS 태깅으로 불용어 처리 \n",
    "\n",
    "- IN: 전치사 또는 종속 접속사\n",
    "- CC: 등위 접속사\n",
    "- UH: 감탄사\n",
    "- TO: 전치사 \"to\"\n",
    "- MD: 조동사\n",
    "- DT: 한정사\n",
    "- VBZ: 동사, 3인칭 단수 현재형\n",
    "- VBP: 동사, 비 3인칭 단수 현재형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/ijinseong/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 품사태깅 패키지 다운로드 \n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 불용어 품사 정의\n",
    "stopPos = ['IN', 'CC', 'UH', 'TO', 'MD', 'DT', 'VBZ','VBP']\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog. \\\n",
    "        The dog barked loudly at the fox.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'The', 'dog', 'barked', 'loudly', 'at', 'the', 'fox', '.']\n"
     ]
    }
   ],
   "source": [
    "# 텍스트를 단어 단위로 토큰화\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('quick', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('jumps', 'VBZ'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('dog', 'NN'),\n",
       " ('barked', 'VBD'),\n",
       " ('loudly', 'RB'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('fox', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어에 대해 품사 태깅 수행\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('The', 'DT'), 2), (('fox', 'NN'), 2), (('the', 'DT'), 2), (('dog', 'NN'), 2), (('.', '.'), 2), (('quick', 'JJ'), 1), (('brown', 'NN'), 1), (('jumps', 'VBZ'), 1), (('over', 'IN'), 1), (('lazy', 'JJ'), 1), (('barked', 'VBD'), 1), (('loudly', 'RB'), 1), (('at', 'IN'), 1)]\n"
     ]
    }
   ],
   "source": [
    "# 최빈어 조회\n",
    "# Counter : 데이터의 빈도수를 계산해서 딕셔너리로 반환\n",
    "# most_commen : Counter 객체의 메서드로, 빈도수가 높은 순서대로 데이터를 정렬한 리스트 반환\n",
    "\n",
    "print(Counter(tagged_tokens).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'lazy',\n",
       " 'dog',\n",
       " '.',\n",
       " 'dog',\n",
       " 'barked',\n",
       " 'loudly',\n",
       " 'fox',\n",
       " '.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 처리: 특정 품사 태그에 해당하는 단어만 필터링\n",
    "stop_words = [word for word, tag in tagged_tokens if tag in stopPos]\n",
    "filtered_tokens = [ word for word, tag in tagged_tokens if word not in stop_words]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사 태깅 결과 :  [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.'), ('The', 'DT'), ('dog', 'NN'), ('barked', 'VBD'), ('loudly', 'RB'), ('at', 'IN'), ('the', 'DT'), ('fox', 'NN'), ('.', '.')]\n",
      "불용어로 간주된 단어들 :  ['The', 'jumps', 'over', 'the', 'The', 'at', 'the']\n",
      "불용어로 삭제된 토큰들 :  ['quick', 'brown', 'fox', 'lazy', 'dog', '.', 'dog', 'barked', 'loudly', 'fox', '.']\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력\n",
    "print(\"품사 태깅 결과 : \", tagged_tokens)\n",
    "print(\"불용어로 간주된 단어들 : \", stop_words)\n",
    "print(\"불용어로 삭제된 토큰들 : \", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 nltk 불용어 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/ijinseong/nltk_data', '/Users/ijinseong/.conda/envs/sesac/nltk_data', '/Users/ijinseong/.conda/envs/sesac/share/nltk_data', '/Users/ijinseong/.conda/envs/sesac/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/Users/{사용자 이름}}/nltk_data', '/Users/ijinseong/nltk_data', '/Users/ijinseong/nltk_data', './Users/ijinseong/nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ijinseong/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk에서 제공 및 정의하는 불용어 목록\n",
    "import nltk\n",
    "\n",
    "# 다운로드 경로 확인\n",
    "print(nltk.data.path)\n",
    "\n",
    "# 불용어 목록 다운\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "print(len(stop_words))\n",
    "print(stop_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'a', 's', 't', 'd', 'm', 'o', 'y']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한 글짜 짜리 불용어 확인\n",
    "[x for x in stop_words if len(x) == 1]   # nltk 토큰화 함수와 어간 추출기를 이용해서 축약형 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 scikit-learn 불용어 목록\n",
    "\n",
    "- 여러개의 불용어 목록을 함께 사용하는 방법도 있음\n",
    "    - 자연어 텍스트의 정보를 얼마나 폐기할 것인가에 따라서 여러 불용어의 합집합(중복제외)이나 교집합(중복)을 사용할 수 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/ijinseong/.conda/envs/sesac/lib/python3.10/site-packages (from scikit-learn) (2.2.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/ijinseong/.conda/envs/sesac/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.0-cp310-cp310-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-macosx_14_0_arm64.whl (23.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.0 scipy-1.14.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n",
      "179\n",
      "\n",
      "378\n",
      "119\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "\n",
    "print(len(sklearn_stop_words))\n",
    "print(len(stop_words))\n",
    "print()\n",
    "\n",
    "stop_words2 = set(stop_words)\n",
    "print(len(stop_words2.union(sklearn_stop_words)))   # 합집합\n",
    "print(len(stop_words2.intersection(sklearn_stop_words)))   # 교집합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 정규화 (원형 복원)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 대소문자 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['house', 'visitor', 'center']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['House', 'Visitor', 'Center']\n",
    "\n",
    "# lower메소드로 정규화 \n",
    "normalized_tokens = [x. lower() for x in tokens]\n",
    "print(normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 어간 추출 (stemming)\n",
    "\n",
    "- 규칙에 기반 하여 토큰을 표준화\n",
    "- ning제거, ful 제거 등\n",
    "\n",
    "- [nltk.stem](https://www.nltk.org/api/nltk.stem.html) 패키지\n",
    "\n",
    "- [규칙상세](https://tartarus.org/martin/PorterStemmer/def.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.1 정규식 사용 \n",
    "\n",
    "- 어간 추출을 위한 정규 표현식 : ^(.*ss\\|.\\*?)(s)?$\n",
    "    - ^ 와 $: 문자열 시작과 끝 -> 즉, 문자열 패턴 전체를 검사 \n",
    "    - 첫 번째 (...) : 어간 추출\n",
    "        - .*ss: ss로 끝나는 문자열 \n",
    "        - | : or 연산자\n",
    "        - .*? : 최소 반복을 사용하여 가능한 한 짧은 문자열 \n",
    "        - 즉, ss로 끝나는 문자열은 유지, 그렇지 않은 단어는 가능한 한 짧게 어간으로 나눔 \n",
    "    - 두 번째 (s)? : 단어 끝에 있는 단일 s 찾음 \n",
    "        - ? : 이 그룹은 선택적임을 의미 (있을 수도 있고 없을 수도 있고)\n",
    "        - 즉, 단어 끝에 s가 있으면 이 부분을 접미사로 분리  \n",
    "\n",
    "- 결과\n",
    "    - 어간과 어미가 분리되어 튜플로 묶여서 리스트에 담김 \n",
    "    - 단어가 둘 이상의 s로 끝나면(ss), 어간은 그 단어 자체 \n",
    "    - 단어가 하나의 s로 끝나면, 어간은 단어에서 s를 제외한 부분, 접미사는 s \n",
    "    - 만일 단어가 s로 끝나지 않으면, 어간은 그 단어 자체이고 접미사는 없음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 접미사 's나 s를 제거 \n",
    "# .findall() : 문자열에서 정규표현식 패턴과 일치하는 모든 문자열을 찾아 리스트로 반환 \n",
    "\n",
    "import re\n",
    "\n",
    "def stem(phrase): \n",
    "    return ' '.join([re.findall('^(.*ss|.*?)(s)?$',\n",
    "                                word)[0][0].strip(\"'\") for word in phrase.lower().split()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house\n",
      "doctor house call\n"
     ]
    }
   ],
   "source": [
    "# 결과 \n",
    "\n",
    "print(stem('houses'))\n",
    "print(stem(\"Doctor House's calls\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Doctor House's call\", 's')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('^(.*ss|.*?)(s)?$', \"Doctor House's calls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'House'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('^(.*ss|.*?)(s)?$', \"House's\")[0][0].strip(\"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.2 NLTK 제공 어간 추출기 사용\n",
    "\n",
    "- 포터 어간추출기는 후행 아포스트로피(')를 유지 \n",
    "- 아래 예제에서는 명시적으로 아포스트로피를 제거해줌 \n",
    "- 소유격 단어와 비소유격 단어를 구분할 필요가 있을 때는 아포스트로피를 유지하는 것이 바람직 \n",
    "- 소유격 형태의 고유명사도 많기 때문에 이런 이름들은 다른 보통 명사와 다르게 취급해야 한다면 아포스트로피를 유지해야 함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dish washer wash dish'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's washed dishes\".split()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dish washer' wash dish\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([stemmer.stem(w) for w in \"dish washer's washed dishes\".split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -> run\n",
      "beautiful -> beauti\n",
      "believes -> believ\n",
      "using -> use\n",
      "conversation -> convers\n",
      "organization -> organ\n",
      "studies -> studi\n"
     ]
    }
   ],
   "source": [
    "# running, beautiful, believes, using, conversation, organization, studies 원형 복원\n",
    "print(\"running -> \" + stemmer.stem(\"running\"))\n",
    "print(\"beautiful -> \" + stemmer.stem(\"beautiful\"))\n",
    "print(\"believes -> \" + stemmer.stem(\"believes\"))\n",
    "print(\"using -> \" + stemmer.stem(\"using\"))\n",
    "print(\"conversation -> \" + stemmer.stem(\"conversation\"))\n",
    "print(\"organization -> \" + stemmer.stem(\"organization\"))\n",
    "print(\"studies -> \" + stemmer.stem(\"studies\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 표제어 추출 \n",
    "\n",
    "- 품사정보를 보존하여 토큰을 표준화\n",
    "\n",
    "- [nlt.stem](http://www.nltk.org/api/nltk.stem.html?highlight=lemmatizer) 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3.1 nltk의 WordNet \n",
    "\n",
    "- WordNet은 영어 어휘 데이터베이스로, 단어들의 의미와 그들 간의 관계를 정리한 큰 사전\n",
    "    - 단어 의미 그래프에 있는 단어 연결관계들만 사용\n",
    "    - 문맥은 고려하지 않음 \n",
    "    - 문맥을 고려할 경우 spaCy같은 더 정교한 알고리즘을 사용해야 함 \n",
    "- 동의어 집합(synsets), 반의어(antonyms), 상위어(hypernyms), 하위어(hyponyms) 등의 의미적 관계를 포함\n",
    "- WordNet은 NLP 작업에서 단어의 의미를 이해하고 단어 간의 관계를 분석하는 데 유용\n",
    "    - 예를 들어, \"dog\"이라는 단어의 동의어와 그와 관련된 개념들을 WordNet을 통해 찾을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ijinseong/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'better'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer.lemmatize('better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('better', pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('good', pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordNet은 goods를 good의 복수형으로 간주\n",
    "lemmatizer.lemmatize('goods', pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goods'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('goods', pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goodness'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('goodness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best 와 good의 연결관계가 없어서 best가 그대로 나옴\n",
    "\n",
    "lemmatizer.lemmatize('best' , pos = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동사 형태 : running -> run\n",
      "형용사 형태 : beautiful -> beautiful\n",
      "명사 형태 : geese -> goose\n"
     ]
    }
   ],
   "source": [
    "# 품사를 명시하여 lemmatize 적용\n",
    "from nltk.corpus import wordnet\n",
    "print(\"동사 형태 : running -> \" + lemmatizer.lemmatize(\"running\", pos=wordnet.VERB))  # 동사 형태로 추출\n",
    "print(\"형용사 형태 : beautiful -> \" + lemmatizer.lemmatize(\"beautiful\", pos=wordnet.ADJ))  # 형용사 형태로 추출\n",
    "print(\"명사 형태 : geese -> \" + lemmatizer.lemmatize(\"geese\", pos=wordnet.NOUN))  # 명사 형태로 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 한국어 전처리 실습 \n",
    "\n",
    "한글의 경우 한글에 맞는 토크나이저를 사용해야 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 한글 토큰화 및 형태소 분석 \n",
    "## konlpy\n",
    "- KoNLPy(코엔엘파이)는 파이썬에서 한국어 자연어 처리를 위한 라이브러리이다.\n",
    "- 한국어 텍스트를 분석하고 처리하는 데 필요한 다양한 도구와 기능을 제공한다.\n",
    "- KoNLPy는 형태소 분석, 품사 태깅, 단어 토크나이징, 구문 분석 등을 수행할 수 있으며, 여러 형태소 분석기(예: Hannanum, Kkma, Komoran, Mecab, Okt 등)를 지원하고 있음.\n",
    "- [토크나이저별 성능/시간 비교](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/#pos-tagging-with-konlpy)\n",
    "\n",
    "## Java 설치\n",
    "- https://www.oracle.com/kr/java/technologies/downloads/#jdk23-mac\n",
    "- 위 사이트에서 운영체제에 맞게 java 설치 \n",
    "- 참고: https://alluring-parent-4dd.notion.site/Java-16dd791a37c68092a949d4076dc65100?pvs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /Users/ijinseong/.conda/envs/sesac/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /Users/ijinseong/.conda/envs/sesac/lib/python3.10/site-packages (from konlpy) (1.5.1)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /Users/ijinseong/.conda/envs/sesac/lib/python3.10/site-packages (from konlpy) (5.3.0)\n",
      "Requirement already satisfied: numpy>=1.6 in /Users/ijinseong/.conda/envs/sesac/lib/python3.10/site-packages (from konlpy) (2.2.0)\n",
      "Requirement already satisfied: packaging in /Users/ijinseong/.conda/envs/sesac/lib/python3.10/site-packages (from JPype1>=0.7.0->konlpy) (24.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 코모란 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 코모란(Komoran) 토큰화\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "komoran_tokens = komoran.morphs(kor_text)\n",
    "print(komoran_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한나눔 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인간', '이', '컴퓨터', '와', '대화', '하고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 한나눔(Hannanum) 토큰화\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "hannanum_tokens = hannanum.morphs(kor_text)\n",
    "print(hannanum_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- okt 토큰화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인간', '이', '컴퓨터', '와', '대화', '하고', '있다는', '것', '을', '깨닫지', '못', '하고', '인간', '과', '대화', '를', '계속', '할', '수', '있다면', '컴퓨터', '는', '지능', '적', '인', '것', '으로', '간주', '될', '수', '있습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# Okt 토큰화\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "okt_tokens = okt.morphs(kor_text)\n",
    "print(okt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kkma 토큰화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kkma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkonlpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Kkma\n\u001b[1;32m      4\u001b[0m Kkma \u001b[38;5;241m=\u001b[39m Kkma()\n\u001b[0;32m----> 5\u001b[0m kkma_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mkkma\u001b[49m\u001b[38;5;241m.\u001b[39mmorphs(kor_text)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(kkma_tokens)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kkma' is not defined"
     ]
    }
   ],
   "source": [
    "# Kkma 토큰화\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "Kkma = Kkma()\n",
    "kkma_tokens = kkma.morphs(kor_text)\n",
    "print(kkma_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. 한글 품사 부착 (PoS Tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 코모란 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'komoran_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 코모란(Komoran) 품사 태깅\u001b[39;00m\n\u001b[1;32m      2\u001b[0m komoran_tag \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mkomoran_tokens\u001b[49m:\n\u001b[1;32m      4\u001b[0m     komoran_tag \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m komoran\u001b[38;5;241m.\u001b[39mpos(token)\n\u001b[1;32m      6\u001b[0m ptint(komoran_tag)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'komoran_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# 코모란(Komoran) 품사 태깅\n",
    "komoran_tag = []\n",
    "for token in komoran_tokens:\n",
    "    komoran_tag += komoran.pos(token)\n",
    "\n",
    "ptint(komoran_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한나눔 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('인간', 'N'), ('이', 'M'), ('컴퓨터', 'N'), ('와', 'I'), ('대화', 'N'), ('하', 'P'), ('고', 'E'), ('있', 'N'), ('다', 'M'), ('는', 'J'), ('것', 'N'), ('을', 'N'), ('깨닫', 'N'), ('지', 'N'), ('못하', 'P'), ('어', 'E'), ('고', 'M'), ('인간', 'N'), ('과', 'N'), ('대화', 'N'), ('를', 'N'), ('계속', 'M'), ('하', 'I'), ('ㄹ', 'N'), ('수', 'N'), ('있', 'N'), ('다면', 'N'), ('컴퓨터', 'N'), ('늘', 'P'), ('ㄴ', 'E'), ('지능적', 'N'), ('이', 'M'), ('ㄴ', 'N'), ('것', 'N'), ('으', 'N'), ('로', 'J'), ('간주', 'N'), ('되', 'N'), ('ㄹ', 'N'), ('수', 'N'), ('있', 'N'), ('슬', 'P'), ('ㅂ니다', 'E'), ('.', 'S')]\n"
     ]
    }
   ],
   "source": [
    "# 한나눔(Hannanum) 품사 태깅\n",
    "hannanum_tag = []\n",
    "for token in hannanum_tokens:\n",
    "    hannanum_tag += hannanum.pos(token)\n",
    "\n",
    "print(hannanum_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- okt 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('인간', 'Noun'), ('이', 'Noun'), ('컴퓨터', 'Noun'), ('와', 'Verb'), ('대화', 'Noun'), ('하고', 'Verb'), ('있다는', 'Adjective'), ('것', 'Noun'), ('을', 'Josa'), ('깨닫지', 'Verb'), ('못', 'Noun'), ('하고', 'Verb'), ('인간', 'Noun'), ('과', 'Noun'), ('대화', 'Noun'), ('를', 'Noun'), ('계속', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있다면', 'Adjective'), ('컴퓨터', 'Noun'), ('는', 'Verb'), ('지능', 'Noun'), ('적', 'Noun'), ('인', 'Noun'), ('것', 'Noun'), ('으로', 'Josa'), ('간주', 'Noun'), ('될', 'Verb'), ('수', 'Noun'), ('있습니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# Okt 품사 태깅\n",
    "okt_tag = []\n",
    "for token in okt_tokens:\n",
    "    okt_tag += okt.pos(token)\n",
    "\n",
    "print(okt_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kkma 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kkma_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Kkma 품사 태깅\u001b[39;00m\n\u001b[1;32m      2\u001b[0m kkma_tag \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mkkma_tokens\u001b[49m:\n\u001b[1;32m      4\u001b[0m     kkma_tag \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m kkma\u001b[38;5;241m.\u001b[39mpos(token)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(kkma_tag)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kkma_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# Kkma 품사 태깅\n",
    "kkma_tag = []\n",
    "for token in kkma_tokens:\n",
    "    kkma_tag += kkma.pos(token)\n",
    "\n",
    "print(kkma_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. 불용어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있습니다.\"\n",
    "\n",
    "# 불용어 리스트 정의\n",
    "stopwords = ['이', '있', '하', '것', '들', '그', '되', '수', '않', \\\n",
    "             '없', '나', '우리', '가', '한', '같', '때', '년', '에', \\\n",
    "             '와', '고', '로', '를', '으로', '에게', '및', '의', '를', \\\n",
    "             '은', '는', '에', '도', '가', '을', '이다', '다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 텍스트를 형태소 단위로 토큰화\n",
    "kkma_tokens = Kkma.morphs(kor_text)\n",
    "print(kkma_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "filtered_tokens = [token for token in kkma_tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 분석 결과: ['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '깨닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '습니다', '.']\n",
      "불용어 제거 후 토큰: ['인간', '컴퓨터', '대화', '다는', '깨닫', '지', '못하', '인간', '과', '대화', '계속', 'ㄹ', '다면', '컴퓨터', '지능', '적', 'ㄴ', '간주', 'ㄹ', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력\n",
    "print(\"형태소 분석 결과:\", kkma_tokens)\n",
    "print(\"불용어 제거 후 토큰:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
